{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgn0WRLRCaNZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2cEHzrs-J8RY"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import tqdm\n",
        "import math\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions to run pose estimation with MoveNet\n",
        "\n",
        "#@markdown You'll download the MoveNet Thunder model from [TensorFlow Hub](https://www.google.com/url?sa=D&q=https%3A%2F%2Ftfhub.dev%2Fs%3Fq%3Dmovenet), and reuse some inference and visualization logic from the [MoveNet Raspberry Pi (Python)](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi) sample app to detect landmarks (ear, nose, wrist etc.) from the input images.\n",
        "\n",
        "#@markdown *Note: You should use the most accurate pose estimation model (i.e. MoveNet Thunder) to detect the keypoints and use them to train the pose classification model to achieve the best accuracy. When running inference, you can use a pose estimation model of your choice (e.g. either MoveNet Lightning or Thunder).*\n",
        "\n",
        "# Download model from TF Hub and check out inference code from GitHub\n",
        "!wget -q -O movenet_thunder.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "!git clone https://github.com/tensorflow/examples.git\n",
        "pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\n",
        "sys.path.append(pose_sample_rpi_path)\n",
        "\n",
        "# Load MoveNet Thunder model\n",
        "import utils\n",
        "from data import BodyPart\n",
        "from ml import Movenet\n",
        "movenet = Movenet('movenet_thunder')\n",
        "\n",
        "# Define function to run pose estimation using MoveNet Thunder.\n",
        "# You'll apply MoveNet's cropping algorithm and run inference multiple times on\n",
        "# the input image to improve pose estimation accuracy.\n",
        "def detect(input_tensor, inference_count=3):\n",
        "  \"\"\"Runs detection on an input image.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: A [height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "    inference_count: Number of times the model should run repeatly on the\n",
        "      same input image to improve detection accuracy.\n",
        "\n",
        "  Returns:\n",
        "    A Person entity detected by the MoveNet.SinglePose.\n",
        "  \"\"\"\n",
        "  image_height, image_width, channel = input_tensor.shape\n",
        "\n",
        "  # Detect pose using the full input image\n",
        "  movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
        "\n",
        "  # Repeatedly using previous detection result to identify the region of\n",
        "  # interest and only croping that region to improve detection accuracy\n",
        "  for _ in range(inference_count - 1):\n",
        "    person = movenet.detect(input_tensor.numpy(),\n",
        "                            reset_crop_region=False)\n",
        "\n",
        "  return person"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q02BNOmLoPx",
        "outputId": "eb0489d5-9fcc-4b47-b078-a2b1689884d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'examples'...\n",
            "remote: Enumerating objects: 23600, done.\u001b[K\n",
            "remote: Counting objects: 100% (560/560), done.\u001b[K\n",
            "remote: Compressing objects: 100% (336/336), done.\u001b[K\n",
            "remote: Total 23600 (delta 164), reused 509 (delta 144), pack-reused 23040\u001b[K\n",
            "Receiving objects: 100% (23600/23600), 44.09 MiB | 30.38 MiB/s, done.\n",
            "Resolving deltas: 100% (12814/12814), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lineA(person_pose):\n",
        "  nose= person_pose.keypoints[0].coordinate\n",
        "  left_shoulder = person_pose.keypoints[5].coordinate\n",
        "  return nose_leftshoulder_line = np.array(nose+left_shoulder)\n",
        "def lineB(person_pose):\n",
        "  left_shoulder = person_pose.keypoints[5].coordinate\n",
        "  left_hip = person_pose.keypoints[11].coordinate\n",
        "  return leftshoulder_lefthip_line= np.array(left_shoulder+left_hip)"
      ],
      "metadata": {
        "id": "n6yTTIuCFZXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions to visualize the pose estimation results.\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, person, crop_region=None, close_figure=True,\n",
        "    keep_input_size=False):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: An numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    person: A person entity returned from the MoveNet.SinglePose model.\n",
        "    close_figure: Whether to close the plt figure after the function returns.\n",
        "    keep_input_size: Whether to keep the size of the input image.\n",
        "\n",
        "  Returns:\n",
        "    An numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  # Draw the detection result on top of the image.\n",
        "  image_np = utils.visualize(image, [person])\n",
        "\n",
        "  # Plot the image with detection results.\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  im = ax.imshow(image_np)\n",
        "\n",
        "  if close_figure:\n",
        "    plt.close(fig)\n",
        "\n",
        "  if not keep_input_size:\n",
        "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
        "\n",
        "  return image_np"
      ],
      "metadata": {
        "id": "vrHyDSluKC47"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to determine if pose is bad or good"
      ],
      "metadata": {
        "id": "eYLf2IdFFtSx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_Fe518uLCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def dot(vA, vB):\n",
        "    return vA[0]*vB[0]+vA[1]*vB[1]\n",
        "\n",
        "def angle(person_pose):\n",
        "    # Get nicer vector form\n",
        "    #vA = [(lineA[0][0]-lineA[1][0]), (lineA[0][1]-lineA[1][1])]\n",
        "    #vB = [(lineB[0][0]-lineB[1][0]), (lineB[0][1]-lineB[1][1])]\n",
        "\n",
        "    nose= person_pose.keypoints[0].coordinate\n",
        "    left_shoulder = person_pose.keypoints[5].coordinate\n",
        "    nose_leftshoulder_line = np.array(nose+left_shoulder)\n",
        "\n",
        "    left_shoulder = person_pose.keypoints[5].coordinate\n",
        "    left_hip = person_pose.keypoints[11].coordinate\n",
        "    leftshoulder_lefthip_line= np.array(left_shoulder+left_hip)\n",
        "\n",
        "    lineA = nose_leftshoulder_line\n",
        "    lineB = leftshoulder_lefthip_line\n",
        "    vA = [(lineA[0]-lineA[2]), (lineA[1]-lineA[3])]\n",
        "    vB = [(lineB[0]-lineB[2]), (lineB[1]-lineB[3])]\n",
        "\n",
        "    # Get dot prod\n",
        "    dot_prod = dot(vA, vB)\n",
        "    # Get magnitudes\n",
        "    magA = dot(vA, vA)**0.5\n",
        "    magB = dot(vB, vB)**0.5\n",
        "    # Get cosine value\n",
        "    cos_ = dot_prod/magA/magB\n",
        "    # Get angle in radians and then convert to degrees\n",
        "    angle = math.acos(dot_prod/magB/magA)\n",
        "    angle = angle*180/np.pi\n",
        "    return angle\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "FcSpOqMcQuq9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/kitana2505/BadPostureAlert/blob/b8fb31ca000a59dbc8b4ae8fdf22a2c9c5cbc878/assets/IMG_0257.mp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFoN6la0KMY0",
        "outputId": "4ced5e07-448d-4881-b566-d5a7871200d2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-04 14:05:01--  https://github.com/kitana2505/BadPostureAlert/blob/b8fb31ca000a59dbc8b4ae8fdf22a2c9c5cbc878/assets/IMG_0257.mp4\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4609 (4.5K) [text/plain]\n",
            "Saving to: ‘IMG_0257.mp4’\n",
            "\n",
            "\rIMG_0257.mp4          0%[                    ]       0  --.-KB/s               \rIMG_0257.mp4        100%[===================>]   4.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-04 14:05:01 (60.3 MB/s) - ‘IMG_0257.mp4’ saved [4609/4609]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YA6uIX15Mv9T"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(\"./IMG_0257.mp4\")\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "fps = int(cap.get(5))\n",
        "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ukDtI_ScOrVC"
      },
      "outputs": [],
      "source": [
        "# Tùy chỉnh output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "output_video = cv2.VideoWriter('output_video.mp4', fourcc, fps, (frame_width, frame_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gfu9tYTbO6JG"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "while True:\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if not ret:\n",
        "    break # break loop khi chạy hết video\n",
        "  drawed_frame = frame.copy()\n",
        "  '''\n",
        "  # Process frame, thay code của em vào đây\n",
        "  # Ở đây a vẽ một hình tròn bán kính ngẫu nhiên vào video\n",
        "\n",
        "  center = (frame_height // 2, frame_width // 2)\n",
        "  radius = np.random.randint(50, 150)\n",
        "  color = (0, 0, 255)\n",
        "  thickness = -1\n",
        "  cv2.circle(drawed_frame, center, radius, color, thickness)'''\n",
        "\n",
        "  #image = tf.io.decode_png(image)\n",
        "  #image = image[:,:, :3]\n",
        "  person_pose = detect(drawed_frame)\n",
        "  _ = draw_prediction_on_image(drawed_frame.numpy(), person_pose, crop_region=None,\n",
        "                               close_figure=False, keep_input_size=True)\n",
        "  critical_angle = angle(person_pose)\n",
        "\n",
        "  # describe the type of font to be used.\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    # Use putText() method for\n",
        "    # inserting text on video\n",
        "  if critical_angle > 45:\n",
        "\n",
        "    cv2.putText(drawed_frame,\n",
        "                'angle = '+ str(critical_angle) + '. Bad pose detected',\n",
        "                (50, 50),\n",
        "                font, 1,\n",
        "                (0, 255, 255),\n",
        "                2,\n",
        "                cv2.LINE_4)\n",
        "\n",
        "\n",
        "  # Save cái video đã xử lý vào output_video.mp4\n",
        "  output_video.write(drawed_frame)\n",
        "\n",
        "  counter+=1\n",
        "  print(f\"[INFO] Process {counter}/{num_frames} frame...\")\n",
        "\n",
        "# Release video objects\n",
        "cap.release()\n",
        "output_video.release()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXh1yET5XjZC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}